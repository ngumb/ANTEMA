#    This file was created by
#    MATLAB Deep Learning Toolbox Converter for TensorFlow Models.
#    07-Jul-2023 14:20:39

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def create_model(shp):
    data = keras.Input(shape=shp, name="data_unnormalized")
    #data = keras.layers.Normalization(axis=None, name="data_")(data_unnormalized)#data = keras.layers.Normalization(axis=(1,2,3), name="data_")(data_unnormalized)
    conv1_prepadded = layers.ZeroPadding2D(padding=((3,3),(3,3)))(data)
    conv1 = layers.Conv2D(64, (7,7), strides=(2,2), name="conv1_")(conv1_prepadded)
    bn_conv1 = layers.BatchNormalization(epsilon=0.000010, name="bn_conv1_")(conv1)
    conv1_relu = layers.ReLU()(bn_conv1)
    pool1_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(conv1_relu)
    pool1 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(pool1_prepadded)
    res2a_branch2a_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(pool1)
    res2a_branch2a = layers.Conv2D(64, (3,3), name="res2a_branch2a_")(res2a_branch2a_prepadded)
    bn2a_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn2a_branch2a_")(res2a_branch2a)
    res2a_branch2a_relu = layers.ReLU()(bn2a_branch2a)
    res2a_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res2a_branch2a_relu)
    res2a_branch2b = layers.Conv2D(64, (3,3), name="res2a_branch2b_")(res2a_branch2b_prepadded)
    bn2a_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn2a_branch2b_")(res2a_branch2b)
    res2a = layers.Add()([bn2a_branch2b, pool1])
    res2a_relu = layers.ReLU()(res2a)
    res2b_branch2a_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res2a_relu)
    res2b_branch2a = layers.Conv2D(64, (3,3), name="res2b_branch2a_")(res2b_branch2a_prepadded)
    bn2b_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn2b_branch2a_")(res2b_branch2a)
    res2b_branch2a_relu = layers.ReLU()(bn2b_branch2a)
    res2b_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res2b_branch2a_relu)
    res2b_branch2b = layers.Conv2D(64, (3,3), name="res2b_branch2b_")(res2b_branch2b_prepadded)
    bn2b_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn2b_branch2b_")(res2b_branch2b)
    res2b = layers.Add()([bn2b_branch2b, res2a_relu])
    res2b_relu = layers.ReLU()(res2b)
    res3a_branch2a_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res2b_relu)
    res3a_branch2a = layers.Conv2D(128, (3,3), strides=(2,2), name="res3a_branch2a_")(res3a_branch2a_prepadded)
    bn3a_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn3a_branch2a_")(res3a_branch2a)
    res3a_branch2a_relu = layers.ReLU()(bn3a_branch2a)
    res3a_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res3a_branch2a_relu)
    res3a_branch2b = layers.Conv2D(128, (3,3), name="res3a_branch2b_")(res3a_branch2b_prepadded)
    bn3a_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn3a_branch2b_")(res3a_branch2b)
    res3a_branch1 = layers.Conv2D(128, (1,1), strides=(2,2), name="res3a_branch1_")(res2b_relu)
    bn3a_branch1 = layers.BatchNormalization(epsilon=0.000010, name="bn3a_branch1_")(res3a_branch1)
    res3a = layers.Add()([bn3a_branch2b, bn3a_branch1])
    res3a_relu = layers.ReLU()(res3a)
    res3b_branch2a_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res3a_relu)
    res3b_branch2a = layers.Conv2D(128, (3,3), name="res3b_branch2a_")(res3b_branch2a_prepadded)
    bn3b_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn3b_branch2a_")(res3b_branch2a)
    res3b_branch2a_relu = layers.ReLU()(bn3b_branch2a)
    res3b_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res3b_branch2a_relu)
    res3b_branch2b = layers.Conv2D(128, (3,3), name="res3b_branch2b_")(res3b_branch2b_prepadded)
    bn3b_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn3b_branch2b_")(res3b_branch2b)
    res3b = layers.Add()([bn3b_branch2b, res3a_relu])
    res3b_relu = layers.ReLU()(res3b)
    res4a_branch2a = layers.Conv2D(256, (3,3), strides=(2,2), padding="same", name="res4a_branch2a_")(res3b_relu)
    bn4a_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn4a_branch2a_")(res4a_branch2a)
    res4a_branch2a_relu = layers.ReLU()(bn4a_branch2a)
    res4a_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res4a_branch2a_relu)
    res4a_branch2b = layers.Conv2D(256, (3,3), name="res4a_branch2b_")(res4a_branch2b_prepadded)
    bn4a_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn4a_branch2b_")(res4a_branch2b)
    res4a_branch1 = layers.Conv2D(256, (1,1), strides=(2,2), padding="same", name="res4a_branch1_")(res3b_relu)
    bn4a_branch1 = layers.BatchNormalization(epsilon=0.000010, name="bn4a_branch1_")(res4a_branch1)
    res4a = layers.Add()([bn4a_branch2b, bn4a_branch1])
    res4a_relu = layers.ReLU()(res4a)
    res4b_branch2a_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res4a_relu)
    res4b_branch2a = layers.Conv2D(256, (3,3), name="res4b_branch2a_")(res4b_branch2a_prepadded)
    bn4b_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn4b_branch2a_")(res4b_branch2a)
    res4b_branch2a_relu = layers.ReLU()(bn4b_branch2a)
    res4b_branch2b_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(res4b_branch2a_relu)
    res4b_branch2b = layers.Conv2D(256, (3,3), name="res4b_branch2b_")(res4b_branch2b_prepadded)
    bn4b_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn4b_branch2b_")(res4b_branch2b)
    res4b = layers.Add()([bn4b_branch2b, res4a_relu])
    res4b_relu = layers.ReLU()(res4b)
    res5a_branch2a = layers.Conv2D(512, (3,3), padding="same", name="res5a_branch2a_")(res4b_relu)
    bn5a_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn5a_branch2a_")(res5a_branch2a)
    res5a_branch2a_relu = layers.ReLU()(bn5a_branch2a)
    res5a_branch2b = layers.Conv2D(512, (3,3), padding="same", dilation_rate=(2,2), name="res5a_branch2b_")(res5a_branch2a_relu)
    bn5a_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn5a_branch2b_")(res5a_branch2b)
    res5a_branch1 = layers.Conv2D(512, (1,1), name="res5a_branch1_")(res4b_relu)
    bn5a_branch1 = layers.BatchNormalization(epsilon=0.000010, name="bn5a_branch1_")(res5a_branch1)
    res5a = layers.Add()([bn5a_branch2b, bn5a_branch1])
    res5a_relu = layers.ReLU()(res5a)
    res5b_branch2a = layers.Conv2D(512, (3,3), padding="same", dilation_rate=(2,2), name="res5b_branch2a_")(res5a_relu)
    bn5b_branch2a = layers.BatchNormalization(epsilon=0.000010, name="bn5b_branch2a_")(res5b_branch2a)
    res5b_branch2a_relu = layers.ReLU()(bn5b_branch2a)
    res5b_branch2b = layers.Conv2D(512, (3,3), padding="same", dilation_rate=(2,2), name="res5b_branch2b_")(res5b_branch2a_relu)
    bn5b_branch2b = layers.BatchNormalization(epsilon=0.000010, name="bn5b_branch2b_")(res5b_branch2b)
    res5b = layers.Add()([bn5b_branch2b, res5a_relu])
    res5b_relu = layers.ReLU()(res5b)
    aspp_Conv_1 = layers.Conv2D(256, (1,1), padding="same", name="aspp_Conv_1_")(res5b_relu)
    aspp_BatchNorm_1 = layers.BatchNormalization(epsilon=0.000010, name="aspp_BatchNorm_1_")(aspp_Conv_1)
    aspp_Relu_1 = layers.ReLU()(aspp_BatchNorm_1)
    aspp_Conv_2 = layers.Conv2D(256, (3,3), padding="same", dilation_rate=(6,6), name="aspp_Conv_2_")(res5b_relu)
    aspp_BatchNorm_2 = layers.BatchNormalization(epsilon=0.000010, name="aspp_BatchNorm_2_")(aspp_Conv_2)
    aspp_Relu_2 = layers.ReLU()(aspp_BatchNorm_2)
    aspp_Conv_3 = layers.Conv2D(256, (3,3), padding="same", dilation_rate=(12,12), name="aspp_Conv_3_")(res5b_relu)
    aspp_BatchNorm_3 = layers.BatchNormalization(epsilon=0.000010, name="aspp_BatchNorm_3_")(aspp_Conv_3)
    aspp_Relu_3 = layers.ReLU()(aspp_BatchNorm_3)
    aspp_Conv_4 = layers.Conv2D(256, (3,3), padding="same", dilation_rate=(18,18), name="aspp_Conv_4_")(res5b_relu)
    aspp_BatchNorm_4 = layers.BatchNormalization(epsilon=0.000010, name="aspp_BatchNorm_4_")(aspp_Conv_4)
    aspp_Relu_4 = layers.ReLU()(aspp_BatchNorm_4)
    catAspp = layers.Concatenate(axis=-1)([aspp_Relu_1, aspp_Relu_2, aspp_Relu_3, aspp_Relu_4])
    dec_c1 = layers.Conv2D(256, (1,1), name="dec_c1_")(catAspp)
    dec_bn1 = layers.BatchNormalization(epsilon=0.000010, name="dec_bn1_")(dec_c1)
    dec_relu1 = layers.ReLU()(dec_bn1)
    dec_upsample1 = layers.Conv2DTranspose(256, (8,8), strides=(4,4), name="dec_upsample1_")(dec_relu1)
    dec_upsample1 = layers.Cropping2D(cropping=((2,2),(2,2)))(dec_upsample1)
    dec_c2 = layers.Conv2D(48, (1,1), name="dec_c2_")(res2b_relu)
    dec_bn2 = layers.BatchNormalization(epsilon=0.000010, name="dec_bn2_")(dec_c2)
    dec_relu2 = layers.ReLU()(dec_bn2)
    dec_crop1 = layers.Cropping2D(cropping=((0,0),(0,0)))(dec_upsample1)
    dec_cat1 = layers.Concatenate(axis=-1)([dec_relu2, dec_crop1])
    dec_c3 = layers.Conv2D(256, (3,3), padding="same", name="dec_c3_")(dec_cat1)
    dec_bn3 = layers.BatchNormalization(epsilon=0.000010, name="dec_bn3_")(dec_c3)
    dec_relu3 = layers.ReLU()(dec_bn3)
    dec_c4 = layers.Conv2D(256, (3,3), padding="same", name="dec_c4_")(dec_relu3)
    dec_bn4 = layers.BatchNormalization(epsilon=0.000010, name="dec_bn4_")(dec_c4)
    dec_relu4 = layers.ReLU()(dec_bn4)
    scorer = layers.Conv2D(2, (1,1), name="scorer_")(dec_relu4)
    dec_upsample2 = layers.Conv2DTranspose(2, (8,8), strides=(4,4), name="dec_upsample2_")(scorer)
    dec_upsample2 = layers.Cropping2D(cropping=((2,2),(2,2)))(dec_upsample2)
    dec_crop2 = layers.Cropping2D(cropping=((0,0),(0,0)))(dec_upsample2)
    softmax_out = layers.Softmax()(dec_crop2)
    labels = softmax_out

    model = keras.Model(inputs=[data], outputs=[labels])
    return model
